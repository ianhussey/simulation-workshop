---
title: "Simulate the violation of assumption of homogeneity of variances"
author: "Ian Hussey"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    highlight: haddock
    theme: flatly
    toc: yes
    toc_float: yes
---

```{r, include=FALSE}

# set default chunk options
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE)

# disable scientific notation
options(scipen = 999) 

```

```{r}

# dependencies
library(dplyr)
library(tidyr)
library(tibble)
library(purrr)
library(janitor)
library(plotrix) # for std.error
library(knitr)
library(kableExtra)

```

# Practice expand_grid()

```{r}

# temp <- tibble(n_control = rep(c(25, 50), 2500)
#                n_intervention = 25,
#                mean_control = 0,
#                mean_intervention = rep(c(0, .2, .5, .8), 1250)
#                sd_control = 1,
#                sd_intervention = c(1, 3),
#                var_equal = TRUE,
#                iteration = 1:5000)


temp2 <- expand_grid(n_control = c(25, 50), 
                     n_intervention = c(25, 50),
                     iteration = 1:5000)

temp2 <- expand_grid(n_control = c(25, 50), 
                     iteration = 1:5000) |>
  mutate(n_itervention = n_control)

temp2 <- expand_grid(n_control = c(25, 50), 
                     n_intervention = c(25, 50),
                     iteration = 1:5000) |>
  filter(n_control == n_intervention)

```

# Simulating the violation of assumptions of a Student's t-test on statistical significance

```{r}

set.seed(42)

generate_data <- function(n_control, 
                          n_intervention,
                          mean_control,
                          mean_intervention,
                          sd_control,
                          sd_intervention) {
  
  require(dplyr)
  require(tibble)
  
  data <- 
    bind_rows(
      tibble(condition = "control",
             score = rnorm(n = n_control, mean = mean_control, sd = sd_control)),
      tibble(condition = "intervention",
             score = rnorm(n = n_intervention, mean = mean_intervention, sd = sd_intervention))
    ) 
  
  return(data)
}

analyse <- function(data, var_equal) {
  
  res_t_test <- t.test(formula = score ~ condition, 
                       data = data,
                       var.equal = var_equal,
                       alternative = "two.sided")
  
  res <- tibble(p = res_t_test$p.value)
  
  return(res)
}

simulation <- 
  # "using the experiment parameters..."
  expand_grid(n_control = 1:500,
              n_intervention = c(25, 50),
              mean_control = 0,
              mean_intervention = 0,
              sd_control = 1,
              sd_intervention = c(1, 3),
              var_equal = c(TRUE, FALSE),
              iteration = 1:5000) |>
  
  # ...generate data that meets those parameters...
  mutate(data = pmap(list(n_control, 
                          n_intervention,
                          mean_control,
                          mean_intervention,
                          sd_control,
                          sd_intervention),
                     generate_data)) |>
  
  # "... then apply the analysis function to the generated data using the parameters relevant to analysis"
  mutate(results = pmap(list(data,
                             var_equal), 
                        analyse))

# summarize across iterations
simulation_results <- simulation |>
  unnest(results) |>
  # ensure all manipulated variables are in the group_by()
  group_by(n_control,
           n_intervention,
           mean_control,
           mean_intervention,
           sd_control,
           sd_intervention,
           var_equal) |>
  summarize(proportion_significant = mean(p < .05),
            se_proportion_significant = std.error(p < .05),
            .groups = "drop") |>
  select(mean_control, mean_intervention, sd_control, sd_intervention, n_control, n_intervention, var_equal,
         proportion_significant, se_proportion_significant) 

# print table
simulation_results |>
  mutate_if(is.numeric, round_half_up, digits = 2) |>
  kable(align = "r") |>
  kable_classic(full_width = FALSE)


beepr:
```

# Making conclusions

Is the above simulation enough to make (general) conclusions about what conditions inflate the false positive rate? How could you make the conclusions more replicable and general?

# Monte Carlo Standard Error

Number of iterations in a simulation study is comparable to number of participants in a real-world study. Best reporting practices in simulation studies (e.g., Siepe et al. 2024) are to consider the precision of Monte Carlo estimates, e.g., using the Standard Error.

Change the number of iterations and observe the impact on the SE and the stability of the estimates (given that no seed was set).

# Discussion points

add set seed

add var_equal FALSE

# Handle acceptable errors

```{r}

possibly_analyse <- purrr::possibly(analyse, otherwise = NA)

# purrr::safely()

```

# Session info

```{r}

sessionInfo()

```


